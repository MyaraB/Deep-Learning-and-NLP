{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural_Network.ipynb","provenance":[],"authorship_tag":"ABX9TyNxhDlvwHpcyHlD34yWR5Fz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V7jQVlX7RPF0","executionInfo":{"status":"ok","timestamp":1638809481503,"user_tz":-120,"elapsed":272640,"user":{"displayName":"Ben Myara","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06316613701850036439"}},"outputId":"8e683f65-ead9-46e1-9d41-fd9d866829b7"},"source":["%matplotlib inline\n","import csv\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow.compat.v1 as tf\n","\n","tf.disable_v2_behavior()\n","\n","#load the data set + convert it to type numpy compatible with tensorflow\n","url = 'https://raw.githubusercontent.com/MyaraB/Deep-Learning-and-NLP/main/dataset_full.csv'\n","df1 = pd.read_csv(url)\n","df = pd.DataFrame(df1)\n","df = df.astype('float32')\n","df=df.to_numpy()\n","\n","#shuffle data\n","np.random.shuffle(df)\n","\n","#manually split the data into 4sets + 1 validation set\n","training_set1 = df[0:14080]\n","test_set1 = df[14080:17600]\n","training_set2 = df[17600:31680]\n","test_set2 = df[31680:35200]\n","training_set3 = df[35200:49280]\n","test_set3 = df[49280:52800]\n","training_set4 = df[52800:66880]\n","test_set4 = df[66880:70400]\n","validation_set= df[70400:] \n","\n","#manually choosing 14 relevant features from the 111 available to us in the dataset\n","#features used: qty_dot_url (1), qty_hyphen_url(2), qty_underline_url(3)\n","#,qty_slash_url(4),qty_equal_url(6), qty_at_url(7),qty_and_url(8), \n","#qty_percent_url(17),qty_tld_url(18), length_url(19),qty_dot_domain(20),\n","#qty_hyphen_domain(21), qty_vowels_domain(37), domain_length(38),phishing(111)\n","\n","training_set1_features = np.array(training_set1[:,[0,1,2,3,5,6,7,16,17,18,19,20,36,37]])\n","training_set1_labels = np.array(training_set1[:,-1])\n","\n","test_set1_features = np.array(test_set1[:,[0,1,2,3,5,6,7,16,17,18,19,20,36,37]])\n","test_set1_labels = np.array(test_set1[:,-1])\n","\n","features = 14\n","eps = 1e-12\n","learning_rates = 0.002\n","\n","#shaping the data in-order for it to be usable with tensor\n","data_x = training_set1_features\n","test_x = test_set1_features\n","data_y = training_set1_labels.reshape((-1,1))\n","test_y = test_set1_labels.reshape((-1,1))\n","\n","learning_rate = 0.002\n","features =14\n","hidden1_size=196\n","training_epochs = 14000\n","\n","#pre proccesing..\n","x = tf.placeholder(tf.float32, [None, features])\n","y_ = tf.placeholder(tf.float32, [None, 1])\n","W1 = tf.Variable(tf.truncated_normal([features, hidden1_size], stddev=0.1))\n","b1 = tf.Variable(tf.constant(0.1, shape=[hidden1_size]))\n","z1 = tf.nn.relu(tf.matmul(x,W1)+b1)\n","W2 = tf.Variable(tf.truncated_normal([hidden1_size, 1], stddev=0.1))\n","b2 = tf.Variable(tf.constant(0.1, shape=[1]))\n","\n","y = tf.nn.sigmoid(tf.matmul(z1,W2)+b2)\n","\n","loss=tf.reduce_mean(-(y_*tf.log(y + eps) + (1 - y_+eps)*tf.log(1 - y+eps)))\n","update = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n","\n","prediction = tf.round(tf.sigmoid(tf.matmul(z1,W2)+b2))\n","correct = tf.cast(tf.equal(prediction, y_), dtype=tf.float32) \n","accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","for epoch in range(0,training_epochs+1):\n","  sess.run(update, feed_dict = {x:data_x, y_:data_y})\n","  if (epoch+1)%1000==0:\n","    err, _ = sess.run([loss, update], {x: data_x, y_:data_y})\n","    feeds_train = {x:data_x, y_:data_y}\n","    feeds_test = {x:test_x, y_:test_y}\n","    train_acc = sess.run(accuracy, feed_dict=feeds_train)\n","    test_acc = sess.run(accuracy, feed_dict=feeds_test)\n","    print (\"epoch: %3d train accuracy: %.3f test accuracy: %.3f loss: %.3f\" % (epoch+1,train_acc, test_acc, err))\n"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1000 train accuracy: 0.890 test accuracy: 0.893 loss: 0.312\n","epoch: 2000 train accuracy: 0.895 test accuracy: 0.897 loss: 0.291\n","epoch: 3000 train accuracy: 0.896 test accuracy: 0.897 loss: 0.280\n","epoch: 4000 train accuracy: 0.896 test accuracy: 0.897 loss: 0.273\n","epoch: 5000 train accuracy: 0.896 test accuracy: 0.898 loss: 0.267\n","epoch: 6000 train accuracy: 0.895 test accuracy: 0.898 loss: 0.262\n","epoch: 7000 train accuracy: 0.895 test accuracy: 0.898 loss: 0.258\n","epoch: 8000 train accuracy: 0.894 test accuracy: 0.899 loss: 0.255\n","epoch: 9000 train accuracy: 0.895 test accuracy: 0.900 loss: 0.252\n","epoch: 10000 train accuracy: 0.895 test accuracy: 0.900 loss: 0.249\n","epoch: 11000 train accuracy: 0.896 test accuracy: 0.901 loss: 0.247\n","epoch: 12000 train accuracy: 0.897 test accuracy: 0.901 loss: 0.245\n","epoch: 13000 train accuracy: 0.897 test accuracy: 0.902 loss: 0.243\n","epoch: 14000 train accuracy: 0.897 test accuracy: 0.901 loss: 0.241\n"]}]}]}