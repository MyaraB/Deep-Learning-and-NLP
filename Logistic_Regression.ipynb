{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Logistic_Regression.ipynb","provenance":[{"file_id":"1AitxXdhk_cpq-4xCRBL0TpfJpIfMzm0p","timestamp":1638618455083}],"collapsed_sections":[],"authorship_tag":"ABX9TyNJt7Npc9gHBRuPMN3TSn2l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34BBs89O8L-o","executionInfo":{"status":"ok","timestamp":1638808727601,"user_tz":-120,"elapsed":19639,"user":{"displayName":"Ben Myara","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06316613701850036439"}},"outputId":"4d4d77fa-6838-4645-9e42-756e2c011b04"},"source":["%matplotlib inline\n","import csv\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow.compat.v1 as tf\n","\n","tf.disable_v2_behavior()\n","\n","#load the data set + convert it to type numpy compatible with tensorflow\n","url = 'https://raw.githubusercontent.com/MyaraB/Deep-Learning-and-NLP/main/dataset_full.csv'\n","df1 = pd.read_csv(url)\n","df = pd.DataFrame(df1)\n","df = df.astype('float32')\n","df=df.to_numpy()\n","\n","#shuffle data\n","np.random.shuffle(df)\n","\n","#manually split the data into 4sets + 1 validation set\n","training_set1 = df[0:14080]\n","test_set1 = df[14080:17600]\n","training_set2 = df[17600:31680]\n","test_set2 = df[31680:35200]\n","training_set3 = df[35200:49280]\n","test_set3 = df[49280:52800]\n","training_set4 = df[52800:66880]\n","test_set4 = df[66880:70400]\n","validation_set= df[70400:] \n","\n","#manually choosing 14 relevant features from the 111 available to us in the dataset\n","#features used: qty_dot_url (1), qty_hyphen_url(2), qty_underline_url(3)\n","#,qty_slash_url(4),qty_equal_url(6), qty_at_url(7),qty_and_url(8), \n","#qty_percent_url(17),qty_tld_url(18), length_url(19),qty_dot_domain(20),\n","#qty_hyphen_domain(21), qty_vowels_domain(37), domain_length(38),phishing(111)\n","\n","training_set1_features = np.array(training_set1[:,[0,1,2,3,5,6,7,16,17,18,19,20,36,37]])\n","training_set1_labels = np.array(training_set1[:,-1])\n","test_set1_features = np.array(test_set1[:,[0,1,2,3,5,6,7,16,17,18,19,20,36,37]])\n","test_set1_labels = np.array(test_set1[:,-1])\n","\n","features = 14\n","eps = 1e-12\n","learning_rates = 0.002\n","\n","#shaping the data in-order for it to be usable with tensor\n","data_x = training_set1_features\n","data_y = training_set1_labels.reshape((-1,1))\n","test_set1_labels = test_set1_labels.reshape((-1,1))\n","\n","x = tf.placeholder(tf.float32,[None,features])\n","y_ = tf.placeholder(tf.float32,[None,1])\n","W = tf.Variable(tf.zeros([features,1]))\n","b = tf.Variable(tf.zeros([1]))\n","y = tf.nn.sigmoid(tf.matmul(x,W)+b)\n","\n","loss=tf.reduce_mean(-(y_*tf.log(y + eps) + (1 - y_+eps)*tf.log(1 - y+eps)))\n","\n","update=tf.train.GradientDescentOptimizer(learning_rates).minimize(loss)\n","\n","prediction = tf.round(tf.sigmoid(tf.matmul(x,W)+b))\n","correct = tf.cast(tf.equal(prediction, y_), dtype=tf.float32) \n","accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","for epoch in range(0,14000):\n","    sess.run(update, feed_dict = {x:data_x, y_:data_y}) #BGD \n","\n","    if (epoch+1)%1000==0: #print every 1000 iterations\n","        err, _ = sess.run([loss, update], {x: data_x, y_:data_y})\n","        feeds_train = {x:data_x, y_:data_y}\n","        feeds_test = {x:test_set1_features, y_:test_set1_labels}\n","        train_acc = sess.run(accuracy, feed_dict=feeds_train)\n","        test_acc = sess.run(accuracy, feed_dict=feeds_test)\n","        print (\"epoch: %3d train accuracy: %.3f test accuracy: %.3f loss: %.3f\" % (epoch+1,train_acc, test_acc, err))\n","       "],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1000 train accuracy: 0.861 test accuracy: 0.868 loss: 0.382\n","epoch: 2000 train accuracy: 0.861 test accuracy: 0.867 loss: 0.356\n","epoch: 3000 train accuracy: 0.869 test accuracy: 0.876 loss: 0.340\n","epoch: 4000 train accuracy: 0.872 test accuracy: 0.879 loss: 0.330\n","epoch: 5000 train accuracy: 0.874 test accuracy: 0.879 loss: 0.323\n","epoch: 6000 train accuracy: 0.875 test accuracy: 0.882 loss: 0.317\n","epoch: 7000 train accuracy: 0.875 test accuracy: 0.883 loss: 0.312\n","epoch: 8000 train accuracy: 0.875 test accuracy: 0.882 loss: 0.308\n","epoch: 9000 train accuracy: 0.876 test accuracy: 0.882 loss: 0.305\n","epoch: 10000 train accuracy: 0.876 test accuracy: 0.881 loss: 0.302\n","epoch: 11000 train accuracy: 0.877 test accuracy: 0.882 loss: 0.300\n","epoch: 12000 train accuracy: 0.878 test accuracy: 0.882 loss: 0.297\n","epoch: 13000 train accuracy: 0.879 test accuracy: 0.883 loss: 0.295\n","epoch: 14000 train accuracy: 0.879 test accuracy: 0.883 loss: 0.294\n"]}]},{"cell_type":"code","metadata":{"id":"Wo-b6QacMIAr"},"source":[""],"execution_count":null,"outputs":[]}]}